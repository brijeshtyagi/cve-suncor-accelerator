terraform {
  required_providers {
    azapi = {
      source = "azure/azapi"
    }

    azurerm = {
      source = "hashicorp/azurerm"
    }

    databricks = {
      source = "databricks/databricks"
    }
  }

  backend "azurerm" {}
}

provider "azurerm" {
  features {}
}

data "azurerm_databricks_workspace" "dbw" {
  name                = var.workspace_name
  resource_group_name = var.resource_group_name
}

provider "databricks" {
  host                        = data.azurerm_databricks_workspace.dbw.workspace_url
  azure_workspace_resource_id = data.azurerm_databricks_workspace.dbw.id
}

locals {
  # https://learn.microsoft.com/en-us/azure/databricks/storage/azure-storage#--set-spark-properties-to-configure-azure-credentials-to-access-azure-storage
  spark_conf = {
    "fs.azure.account.auth.type.${var.storage_account_name}.dfs.core.windows.net"              = "OAuth"
    "fs.azure.account.oauth.provider.type.${var.storage_account_name}.dfs.core.windows.net"    = "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"
    "fs.azure.account.oauth2.client.endpoint.${var.storage_account_name}.dfs.core.windows.net" = "https://login.microsoftonline.com/${var.tenant_id}/oauth2/token"
    "fs.azure.account.oauth2.client.id.${var.storage_account_name}.dfs.core.windows.net"       = var.databricks_sp_id
    "fs.azure.account.oauth2.client.secret.${var.storage_account_name}.dfs.core.windows.net"   = "{{secrets/${var.key_vault_name}/${var.key_vault_secret_name}}}"
    "spark.databricks.delta.preview.enabled"                                                   = "true"
  }
}

data "azurerm_key_vault" "kv" {
  name                = var.key_vault_name
  resource_group_name = var.resource_group_name
}

# https://learn.microsoft.com/en-us/azure/databricks/getting-started/connect-to-azure-storage
resource "databricks_secret_scope" "secret_scope" {
  name = var.key_vault_name

  keyvault_metadata {
    resource_id = data.azurerm_key_vault.kv.id
    dns_name    = data.azurerm_key_vault.kv.vault_uri
  }
}

# Create the cluster with the "smallest" amount of resources allowed.
data "databricks_node_type" "smallest" {
  local_disk = true
}

data "databricks_spark_version" "version" {
  spark_version = "3.4.0"
  scala         = "2.12"
}

resource "databricks_cluster" "cluster" {
  cluster_name            = var.cluster_name
  autotermination_minutes = var.cluster_autotermination_minutes
  num_workers             = var.cluster_num_workers
  node_type_id            = data.databricks_node_type.smallest.id
  spark_version           = data.databricks_spark_version.version.id
  spark_conf              = local.spark_conf
  data_security_mode      = var.cluster_data_security_mode

  depends_on = [databricks_secret_scope.secret_scope]
}
