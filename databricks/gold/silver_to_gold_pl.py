# Databricks notebook source
# Install libraries
%pip install pyyaml
%pip install dlt

# COMMAND ----------

# Append the path to the databricks folder to the system path
import sys
sys.path.append('/Workspace/Shared/databricks/')

# COMMAND ----------

# Import libraries
import dlt
from utilities.op_utilities import get_config
from pyspark.sql.functions import col

# COMMAND ----------

# Get the pipeline parameters
config_file_path = spark.conf.get("config_file_path")
params = get_config(config_file_path)

# Silver to gold parameters
silver_to_gold_params = params["silver_to_gold_details"]
silver_schema = silver_to_gold_params["silver_schema"]
silver_table = silver_to_gold_params["silver_table"]
gold_table = silver_to_gold_params["gold_table"]
merge_id = silver_to_gold_params["merge_id"]
logical_order = silver_to_gold_params["logical_order"]

# COMMAND ----------

# Define the temp view
@dlt.view
def silver_table_view():
    return spark.readStream.format("delta").table(f"{silver_schema}.{silver_table}")

# Create gold layer streaming table
dlt.create_streaming_table(gold_table)

# SCD type 2 implementation using databricks' apply_changes() function
dlt.apply_changes(
    target = gold_table,
    source = "silver_table_view",
    keys = [merge_id],
    sequence_by = col(logical_order),
    #except_column_list = [logical_order],
    stored_as_scd_type = "2",
    track_history_except_column_list = [logical_order]
)
