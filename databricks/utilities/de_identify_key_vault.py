# Databricks notebook source
# Install libraries
%pip install pyyaml

# COMMAND ----------

import sys
sys.path.append('/Workspace/Shared/databricks/')
from utilities.op_utilities import get_config, merge_data
from pyspark.sql.functions import sha2, col, udf, lit, md5
from cryptography.fernet import Fernet
from pyspark.sql.types import StringType

# Get the pipeline parameters
config_file_path = dbutils.widgets.get("config_file_path")
params = get_config(config_file_path)

de_identification_params = params["de_identification_details"]

source_schema = de_identification_params["source_schema"]
source_table = f"{source_schema}.{de_identification_params['source_table']}"

de_identification_schema = de_identification_params["de_identification_schema"]
de_identification_table = f"{de_identification_schema}.{de_identification_params['de_identification_table']}"
deidentify_data = de_identification_params["deidentify_data"]
columns_to_tokenize = de_identification_params["deidentify_col"]
de_identification_lookup_table=f"{de_identification_schema}.{de_identification_params['de_identification_lookup_table']}"
secret_scope_name = de_identification_params['secret_scope_name']
secret_key_name = de_identification_params['secret_key_name']
if deidentify_data == "True":
    deidentify_data = True
else:
    deidentify_data = False


# COMMAND ----------



# Define Encrypt User Defined Function 
def encrypt_val(clear_text,MASTER_KEY):
    from cryptography.fernet import Fernet
    f = Fernet(MASTER_KEY)
    clear_text_b=bytes(clear_text, 'utf-8')
    cipher_text = f.encrypt(clear_text_b)
    cipher_text = str(cipher_text.decode('ascii'))
    return cipher_text
 
# Define decrypt user defined function 
def decrypt_val(cipher_text,MASTER_KEY):
    from cryptography.fernet import Fernet
    f = Fernet(MASTER_KEY)
    clear_val=f.decrypt(cipher_text.encode()).decode()
    return clear_val

# Register UDF's
encrypt = udf(encrypt_val, StringType())
decrypt = udf(decrypt_val, StringType())

encryption_key = dbutils.secrets.getBytes(scope = secret_scope_name, key = secret_key_name)



# COMMAND ----------

if deidentify_data:
    # Get the raw silver dataframe
    df = spark.table(source_table)

    # Tokenize the columns using SHA-256 hashing
    for column in columns_to_tokenize:
        column_type = df.schema[column].dataType

        if str(column_type) == "BinaryType":
            # If the column is of BinaryType, hash the values directly
            df = df.withColumn(column + "_original", col(column))
            #df = df.withColumn(column, sha2(col(column), 256))
            df = df.withColumn(column, encrypt(col(column),lit(encryption_key)))
        else:
            # If the column is not BinaryType, convert it to string and then hash
            df = df.withColumn(column + "_original", col(column))
            #df = df.withColumn(column, sha2(col(column).cast("string"), 256))
            df = df.withColumn(column, encrypt(col(column).cast("string"),lit(encryption_key))) 
    # Create a new DataFrame without the original columns
    df_tokenized = df.select(
        [column for column in df.columns if not column.endswith("_original")]
    )

    # Save the de-identified data to the respective tables
    merge_data(df, de_identification_lookup_table, operation="append")
    merge_data(df_tokenized, de_identification_table, operation="append")
