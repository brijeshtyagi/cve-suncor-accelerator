# Module containing functions for data validation
import yaml
import json
from pyspark.shell import spark
from pyspark.sql.functions import (
    lit,
    col,
    count,
    udf,
    create_map,
    monotonically_increasing_id,
    row_number
)
from itertools import chain
import datetime
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.window import Window
from pyspark.sql.types import IntegerType, StringType, StructType, TimestampType

def unique_record_check(df: DataFrame, expectation_details: dict, return_invalid: bool = False) -> DataFrame:
    """ Get the unique/non-unique records based on the specified identifying columns
    Args:
      df: A pyspark DF to run the unique_value_check on
      expectation_details: A dictionary of expectation details with the check_record_unique rules
      return_invalid: Whether to return the valid or invalid records
    Returns:
      df: A pyspark DF with the unique/non-unique records
    """
    for data_check in expectation_details:
      if data_check["data_validation_check"] == "check_record_unique":
        columns = data_check["relevant_columns"]
        window_spec = Window.partitionBy(*columns)
        count_column = count("*").over(window_spec).alias("unique_record_count")
        result_df = df.select('*', count_column)

        if return_invalid == False:
          result_df = result_df.filter("unique_record_count == 1").drop("unique_record_count")

        elif return_invalid == True:
          result_df = result_df.filter("unique_record_count != 1")

        else:
          raise ValueError("Invalid argument for keep_invalid!")

        return result_df
    
@udf(returnType=StringType())
def check_consent_date(df_datefield: TimestampType, opt_in_dt: TimestampType, opt_out_dt: TimestampType):
    """Returns valid or invalid depending on if the input row has valid consent
    Args:
      row: The DataFrame row to check (enter as a struct())
      consent_table_name: The name of the consent table
      consent_table_id: The name of the id column in the consent table
      df_id: The name of the id column in the DF to check
      df_datefield: The name of the datefield column in the DF to check
    Returns:
      consent_check: Valid or Invalid depending on the row"""
    if opt_in_dt is None:
      return "Invalid"
    elif ((opt_in_dt is not None) & (opt_out_dt is None)):
      if df_datefield >= opt_in_dt:
        return "Valid"
      else:
        return "Invalid"
    elif ((opt_in_dt is not None) & (opt_out_dt is not None)):
      if ((df_datefield >= opt_in_dt) & (df_datefield <= opt_out_dt)):
        return "Valid"
      else:
        return "Invalid"
    else: 
      return "Invalid"

def consent_check(consent_table_name: str, df_to_check: DataFrame, consent_table_id: str, df_id: str, df_datefield: str) -> DataFrame:
    """Returns the input dataframe with a new column consent_check
    that is valid or invalid depending on the input consent table
    Args:
      consent_table_name: The name of the table with the consent data
      df_to_check: The pyspark DF to run the consent check on
      consent_table_id: The name of the id column in the consent table
      df_id: The name of the id column in the DF to check
      df_datefield: The name of the datefield column in the DF to check
    Returns:
      df_w_consent_check: A pyspark DF with the consent check applied"""
    spark = SparkSession.builder.getOrCreate()

    consent_table_df = spark.table(consent_table_name)

    full_df = df_to_check.join(consent_table_df, col(df_id) == col(consent_table_id), "left")

    result_df = (full_df
                 .withColumn("consent_check", check_consent_date(col(df_datefield), col("opt_in_dt"), col("opt_out_dt")))
                 .drop(consent_table_id, "opt_in_dt", "opt_out_dt", "opt_out_reason"))

    return result_df
