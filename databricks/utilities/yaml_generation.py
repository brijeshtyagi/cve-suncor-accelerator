# Databricks notebook source
# Install Libraries
%pip install pyyaml

# COMMAND ----------

# Append the path to the databricks folder to the system path
import sys
sys.path.append('/Workspace/Shared/databricks/')

# COMMAND ----------

import yaml
import json
import datetime
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder \
    .appName("AzureSQLConnection") \
    .getOrCreate()

#sql_username = dbutils.secrets.get("my-keyvault-scope", "sql-username")
#sql_password = dbutils.secrets.get("my-keyvault-scope", "sql-password")

# Azure SQL connection details
server = 'sqlsuncoredev.database.windows.net'
database = 'controldb'
username = 'controldb_reader'
password = 'Slalom2023$$'

# JDBC URL for Azure SQL Database
jdbc_url = f"jdbc:sqlserver://{server};database={database};user={username};password={password}"

# Table name to read
table_name = "control.v_YAMLExtractView"

# Read data from the Azure SQL Database table
df = spark.read.jdbc(url=jdbc_properties["url"], table=table_name, properties=jdbc_properties)


for row in df.collect():

    # Generate YAML fields. Treat each fields as dictionary, This part will need to be modifiied based on different requirements
    record_dict = {
        "data_pipeline_name":"Suncor_DBX_ACC_PIPE",
        "bronze_database":"suncor_bronze",
        "bronze_database":"suncor_silver",
        "bronze_database":"suncor_gold",
        "bronze_ingestion_details":{
            "source_file_path":"abfss://"+row.SourceServerName+row.SourceDBName+row.SourceTableName,
            "source_file_format":"csv",
            "source_file_delimiter":"|",
            "source_file_multiline":"", #Only for JSONS
            "bronze_schema":"suncor_bronze",
            "bronze_table":"bronze"+row.SourceTableName
        },
        "silver_ingestion_details":{
            "source_bronze_schema":"suncor_bronze",
            "source_bronze_table":"bronze"+row.SourceTableName,
            "silver_schema":"suncor_silver",
            "silver_table":"silver"+row.SourceTableName
        }
    }

    # Generate YAML content
    yaml_content = yaml.dump(record_dict, default_flow_style=False)

    # Define the file path for the YAML file
    file_path = "/Workspace/Shared/databricks/{row.id}.yaml"

    # Write YAML content to the file
    with open(file_path, 'w') as yaml_file:
        yaml_file.write(yaml_content)


# COMMAND ----------


