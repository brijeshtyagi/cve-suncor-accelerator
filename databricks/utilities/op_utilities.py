# Module containing general functions required for running the DLT pipeline
import yaml
import json
from pyspark.shell import spark
from pyspark.sql.functions import (
    lit,
    col,
    count,
    udf,
    create_map,
    monotonically_increasing_id,
    row_number,
    expr,
    desc,
    when
)
from itertools import chain
from delta.tables import DeltaTable
import datetime
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.window import Window
from pyspark.sql.types import IntegerType, StringType, StructType, TimestampType
import pyspark.sql.functions as F


def get_config(config_file_path: str) -> dict:
    """Load the configurations in the yaml config file.
    Args:
      config_file_path: A string of the config file path
    Returns:
      dict: A dictionary representing the config file"""
    with open(config_file_path, "r") as file:
        config_dict = yaml.load(file, Loader=yaml.FullLoader)

    return config_dict


def get_schema_from_json(json_schema_path):
    """Function to read in json format schema"""
    with open(json_schema_path, "r") as file:
        json_schema = file.read()
    schema = StructType.fromJson(json.loads(json_schema))
    return schema


def get_expectation_and_quarantine_rules(expectation_details: dict) -> dict:
    """Generate a set of expectation and quarantine rules using yaml config.
    This only includes checks that are able to be created using expectation/quarantine_rule_syntax
    Args:
      expectation_details: A dictionary representing the expectation rules in the YAML config
    Return:
      dict: A dictionary representing acceptable DLT expectation and quarantine (the inverse) rules"""
    if expectation_details == None:
        rules = {}
        rules["expectation_rules"] = None
        rules["quarantine_rules"] = None
        return rules

    else:
        expectation_rules = {}
        for data_check in expectation_details:
            if data_check["data_validation_check"] == "check_value_empty":
                for column in data_check["relevant_columns"]:
                    expectation_rules[
                        f"{column} IS NOT NULL"
                    ] = f"({column} IS NOT NULL)"
            elif data_check["data_validation_check"] == "check_value_equal":
                for column_value_pair in data_check["relevant_columns"]:
                    for key, value in column_value_pair.items():
                        expectation_rules[f"{key} equal"] = f"({key} == {value})"
            elif data_check["data_validation_check"] == "check_value_in":
                for column_values_pair in data_check["relevant_columns"]:
                    for key, values in column_values_pair.items():
                        expectation_rules[f"{key} in"] = f"({key} in {tuple(values)})"
            elif (
                data_check["data_validation_check"]
                == "check_value_greater_than_or_equal"
            ):
                for column_value_pair in data_check["relevant_columns"]:
                    for key, value in column_value_pair.items():
                        expectation_rules[
                            f"{key} greater than or equal"
                        ] = f"({key} >= {value})"
            elif (
                data_check["data_validation_check"] == "check_value_less_than_or_equal"
            ):
                for column_value_pair in data_check["relevant_columns"]:
                    for key, value in column_value_pair.items():
                        expectation_rules[
                            f"{key} less than or equal"
                        ] = f"({key} <= {value})"
            elif data_check["data_validation_check"] == "check_value_between":
                for column_values_pair in data_check["relevant_columns"]:
                    for key, values in column_values_pair.items():
                        expectation_rules[
                            f"{key} between"
                        ] = f"(({key} >= {values['min']}) AND ({key} <= {values['max']}))"
            # Unique values can't be checked for using DLT expectations and quarantine rules
            elif data_check["data_validation_check"] == "check_record_unique":
                pass
            else:
                raise ValueError("Invalid value for the date_validation_check!")

        quarantine_rules = "NOT({0})".format(" AND ".join(expectation_rules.values()))
        rules = {}
        rules["expectation_rules"] = expectation_rules
        rules["quarantine_rules"] = quarantine_rules

        return rules


def get_all_data_check_rules(expectation_details: dict) -> list:
    """Retrieve a list of data validation rules"""
    data_check_rules = []
    for data_check in expectation_details:
        data_check_rules.append(data_check["data_validation_check"])

    return data_check_rules


def create_quarantine_records(
    rejected_records: DataFrame, data_stage: str, quarantine_reason: str, dataset: str
) -> DataFrame:
    """create quarantine records as a Dataframe to be append to the table.
    Args:
      rejected_records: records that need to be quarantined
      data_stage: the medallion layer the data is in
      quarantine_reason: why the data was quarantine
      dataset: the name of the dataset
    Returns:
      DataFrame: quarantine_records"""

    error_info = create_map(
        list(
            chain(
                *(
                    (lit(name), col(name).cast(StringType()))
                    for name in rejected_records.columns
                )
            )
        )
    )
    current_time = str(datetime.datetime.now())

    quarantine_records = (
        rejected_records.withColumn("error_record", error_info)
        .withColumn("dataset_state", lit(data_stage))
        .withColumn("dataset_name", lit(dataset))
        .withColumn("quarantine_reason", lit(quarantine_reason))
        .withColumn("quarantine_datetime", lit(current_time))
    )
    quarantine_records = quarantine_records.select(
        "dataset_state",
        "dataset_name",
        "quarantine_reason",
        "quarantine_datetime",
        "error_record",
    )

    return quarantine_records


def send_records_to_quarantine(
    rejected_records: DataFrame, quarantine_database: str, quarantine_table: str, df_type: str
):
    """Function to send records to quarantine"""
    if df_type == "static":
        if spark.catalog.tableExists(quarantine_table):
            rejected_records.write.format("delta").mode("overwrite").saveAsTable(
                quarantine_table
            )
        else:
            rejected_records.write.format("delta").mode("append").option(
                "checkpointLocation", f"{quarantine_database}.check_point_location"
            ).saveAsTable(f"{quarantine_database}.{quarantine_table}")
    elif df_type == "streaming":
        rejected_records.writeStream.format("delta").outputMode("append").option(
            "checkpointLocation", f"{quarantine_database}.check_point_location"
        ).toTable(f"{quarantine_database}.{quarantine_table}")
    else:
        raise ValueError("Invalid df_type entered!")


def perform_scd1_updates(source_df, target_table, merge_columns, update_columns):
    """
    Perform SCD Type 1 updates.

    Args:
    source_df (dataframe): The source DataFrame containing the data to merge.
    target_table (str): Name of the target table (database.tablename).
    merge_columns (list): A list of columns used for merging.
    update_columns (list): A list of columns to update in the target table. Pass an empty list to update all columns.

    Returns:
    None
    """
    # DeltaTable object for the target table
    delta_table = DeltaTable.forName(spark, target_table)

    # The merge condition based on the merge columns
    merge_condition = " AND ".join(
        ["target.{0} = source.{0}".format(col) for col in merge_columns]
    )

    if len(update_columns) == 0:
        # Update all column values and insert all records when no match is found
        delta_table.alias("target").merge(
            source_df.alias("source"), condition=expr(merge_condition)
        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()
    else:
        # Update specific column values and insert all records when no match is found
        delta_table.alias("target").merge(
            source_df.alias("source"), condition=expr(merge_condition)
        ).whenMatchedUpdate(
            set=dict((col, "source.{0}".format(col)) for col in update_columns)
        ).whenNotMatchedInsertAll().execute()


def perform_scd2_updates(source_df, target_table, dt_value, merge_columns):
    """
    Perform SCD Type 2 updates.

    Args:
    source_df (dataframe): The source DataFrame containing the data to merge.
    target_table (str): Name of the target table (database.tablename).
    merge_columns (list): A list of columns used for merging.

    Returns:
    None
    """

    # Access the Delta table
    delta_table = DeltaTable.forName(spark, target_table)

    # Select columns from the source dataframe that are not in the merge columns
    result_columns = [x for x in source_df.columns if x not in merge_columns]

    # Iterate over each column in the result columns, create the filter condition comparing the source and target columns
    filter_condition = None
    for column in result_columns:
        if filter_condition is None:
            filter_condition = col(f"source.{column}") != col(f"target.{column}")
        else:
            filter_condition = filter_condition | (
                col(f"source.{column}") != col(f"target.{column}")
            )

    # Create the join condition by comparing merge columns
    join_condition = " AND ".join(
        f"target.{col} = source.{col}" for col in merge_columns
    )

    # Add an additional condition for target end_date being NULL, to compare source_df records with the latest record in target table
    join_condition += " AND target.end_date IS NULL"

    # Read the target table into a dataframe
    target_df = spark.table(target_table)

    # Perform an inner join between source and target dataframes and apply the filter_condition
    # To find the existing records in the target table that have been chnaged
    modified_records_df = (
        source_df.alias("source")
        .join(target_df.alias("target"), F.expr(join_condition), "inner")
        .filter(filter_condition)
        .selectExpr("source.*")
    )

    # Perform a left anti-join to find new records
    new_records_df = source_df.alias("b").join(
        target_df.alias("target"),
        on=[source_df[col] == target_df[col] for col in merge_columns],
        how="left_anti",
    )

    # Create result dataframe for updated records
    result_df_1 = modified_records_df.alias("source").selectExpr(
        "NULL as mergeKey", "source.*"
    )

    result_df_2 = modified_records_df.unionAll(new_records_df)
    result_df_2 = result_df_2.selectExpr(
        *[f"{col} as mergeKey" for col in merge_columns], "*"
    )

    result_df = result_df_1.unionAll(result_df_2)
    result_df = result_df.withColumn("start_date", dt_value).withColumn(
        "end_date", F.lit(None).cast("timestamp")
    )

    # Perform the merge operation on the delta table to end th existing record and insert new ones
    delta_table.alias("target").merge(
        result_df.alias("b"),
        F.expr(" AND ".join(f"target.{col} = b.mergeKey" for col in merge_columns)),
    ).whenMatchedUpdate(
        condition=F.expr("target.end_date IS NULL"), set={"end_date": dt_value}
    ).whenNotMatchedInsertAll().execute()


def append_data(source_df, target_table):
    """
    Append the source DataFrame to the existing target table.

    Args:
    source_df (dataframe): The source DataFrame containing the data to append.
    target_table (str): Name of the target table (database.tablename).

    Returns:
    None
    """
    source_df.write.format("delta").mode("append").saveAsTable(target_table)


def merge_data(
    source_df, target_table, operation, scd_type="", dt_value=None, merge_columns=[], update_columns=[]
):
    """
    Function to perform upsert/append operations on the source and target data, scd type 1/ scd type 2.

    Args:
    source_df (dataframe): The source DataFrame containing the data to merge.
    target_table (str): Name of the target table (database.tablename).
    operation (str): Operation type, either 'upsert' or 'append'.
    merge_columns (list): A list of columns used for merging.
    update_columns (list): A list of columns to update in the target table. Pass an empty list to update all columns.
    scd_type (str): SCD Type to perform, either 'scd1' or 'scd2'.

    Returns:
    None
    """
    # Check if the target table exists
    if spark.catalog.tableExists(target_table):
        if operation == "upsert":
            # Determine SCD type based on conditions
            if scd_type == "scd1":
                # Perform SCD Type 1 updates
                perform_scd1_updates(
                    source_df, target_table, merge_columns, update_columns
                )
            elif scd_type == "scd2":
                # Perform SCD Type 2 updates
                perform_scd2_updates(source_df, target_table, dt_value, merge_columns)
            else:
                raise ValueError(
                    "Invalid SCD type. Supported types are 'scd1' and 'scd2'."
                )
        elif operation == "append":
            # Append the source DataFrame to the existing target table
            append_data(source_df, target_table)
        else:
            raise ValueError(
                "Invalid operation. Supported operations are 'upsert'( 'scd1', and 'scd2'), 'append'."
            )
    else:
        if scd_type == "scd2":
            source_df_with_dates = source_df.withColumn(
                "start_date", dt_value
            )
            source_df_with_dates = source_df_with_dates.withColumn(
                "end_date", F.lit(None).cast(TimestampType())
            )
            source_df_with_dates.write.format("delta").mode("overwrite").saveAsTable(
                target_table
            )
        else:
            # If the target table does not exist, create it and save the source DataFrame
            source_df.write.format("delta").mode("overwrite").saveAsTable(target_table)


def deduplicate_data(df, partition_columns, datetime_column, return_invalid=False):
    """
    Function to deduplicate data based on specified columns.

    Args:
        df (DataFrame): Input DataFrame containing the data.
        partition_columns (list): List of column names used for partitioning.
        datetime_column (str): Name of the column used for ordering by date.

    Returns:
        DataFrame: Deduplicated DataFrame.

    """
    # Partition by the key and order by datetime column, add a row number column to identify the latest record within each partition
    df_with_row_number = df.withColumn(
        "row_number",
        row_number().over(
            Window.partitionBy(
                [col(col_name) for col_name in partition_columns]
            ).orderBy(df[datetime_column].desc())
        ),
    )

    if return_invalid == False:

        # Filter and select only the latest records within each partition
        result_df = df_with_row_number.filter(col("row_number") == 1).drop("row_number")

    else:
        # Filter and select only the invalid records within each partition
        result_df = df_with_row_number.filter(col("row_number") != 1).drop("row_number")
    return result_df