# Databricks notebook source
# Install libraries
%pip install pyyaml

# COMMAND ----------

from utilities.op_utilities import get_config, merge_data
from pyspark.sql.functions import sha2, col

# Get the pipeline parameters
config_file_path = dbutils.widgets.get("config_file_path")
params = get_config(config_file_path)

de_identification_params = params["de_identification_details"]

silver_schema = de_identification_params["silver_schema"]
silver_table = f"{silver_schema}.{de_identification_params['silver_table']}"

de_identification_schema = de_identification_params["de_identification_schema"]
de_identification_table = f"{de_identification_schema}.{de_identification_params['de_identification_table']}"
deidentify_data = de_identification_params["deidentify_data"]
columns_to_tokenize = de_identification_params["deidentify_col"]
de_identification_lookup_table=f"{de_identification_schema}.{de_identification_params['de_identification_lookup_table']}"

if deidentify_data == "True":
    deidentify_data = True
else:
    deidentify_data = False


# COMMAND ----------

if deidentify_data:
    # Get the raw silver dataframe
    df = spark.table(silver_table)

    # Tokenize the columns using SHA-256 hashing
    for column in columns_to_tokenize:
        column_type = df.schema[column].dataType

        if str(column_type) == "BinaryType":
            # If the column is of BinaryType, hash the values directly
            df = df.withColumn(column + "_original", col(column))
            df = df.withColumn(column, sha2(col(column), 256))
        else:
            # If the column is not BinaryType, convert it to string and then hash
            df = df.withColumn(column + "_original", col(column))
            df = df.withColumn(column, sha2(col(column).cast("string"), 256))

    # Create a new DataFrame without the original columns
    df_tokenized = df.select(
        [column for column in df.columns if not column.endswith("_original")]
    )

    # Save the de-identified data to the respective tables
    merge_data(df, de_identification_lookup_table, operation="append")
    merge_data(df_tokenized, de_identification_table, operation="append")
