# Databricks notebook source
# MAGIC %pip install pydicom

# COMMAND ----------

import hashlib
import datetime
from pyspark.sql.functions import col, udf
from pyspark.sql.types import MapType, StringType
import pydicom
from pydicom.dataset import Dataset
from pydicom.errors import InvalidDicomError
from pyspark.sql import DataFrame
import json

# COMMAND ----------

def replace(s: str, index: int, c: str) -> str:
    """Replace string with specific character.
    Args:
      s: input string which is a file path
      index: position to replace
      c: the character to replace
    Return:
      a new str"""
    chars = list(s)
    chars[index] = c
    res = "".join(chars)
    res = "/" + res
    return res


def read_dicom_file(local_path: str):
    """Read a dicom file using pydicom
    Args:
      str: string represents file path
    Returns:
      If file not corrupt, return the metadata of the file
      If file is corrupt, return the string represent corrupt
    """
    try:
        ds = pydicom.dcmread(local_path)
        return "Valid", ds
    except InvalidDicomError:
        return "Corrupt", None


def dicom_meta(ds) -> str:
    """Extract metadata of dicom file as string
    Args:
      ds: Pydicom dataset
      file_path_dcm: string represent file path
    Returns:
      str: json string represent metadata"""
    metadata = ds.to_json_dict()
    return json.dumps(metadata)


def extract_metadata(file_path_dcm: str) -> dict:
    """Extract metadata from a valid dicom file as a dict.
    Args:
      file_path_dcm: string represent file path
    Returns:
      dict: dictionary represent dicom metadata"""
    now = datetime.datetime.now()
    current_date = str(now.date())
    current_time = str(now.time())
    dicom_file_details = {}
    status, ds = read_dicom_file(file_path_dcm)

    if ds:
        # file is not corrupt
        dicom_file_details["metadata"] = dicom_meta(ds)
        sha1_hash = hashlib.sha1(ds.PixelData).hexdigest()
        dicom_file_details["hash_value"] = sha1_hash

    dicom_file_details["status"] = status
    dicom_file_details["ingestion_timestamp"] = current_time
    dicom_file_details["ingestion_date"] = current_date

    return dicom_file_details


def get_dicom_dir_content(ls_path: str):
    """List all dicom files within a given directory path.
    Args:
      ls_path: str represent directory path
    Yields:
      List of FileObject with dir path"""
    print(dbutils.fs.ls(ls_path))

    for dir_path in dbutils.fs.ls(ls_path):
        if dir_path.isFile() and dir_path.path.endswith(".dcm"):
            yield dir_path
        elif dir_path.isDir() and ls_path != dir_path.path:
            yield from get_dicom_dir_content(dir_path)


def ingest_dicom_files(data_from_config: dict) -> DataFrame:
    """Main function to ingest dicom files using data from config.
    Args:
      data_from_config: details from yaml
    Returns:
      str: string representing whether the process has completed"""

    ingestion_details = data_from_config["ingestion_details"]
    source_file_path = ingestion_details["source_file_path"]
    file_list = list(get_dicom_dir_content(source_file_path))
    print(file_list)
    values = []
    columns = ["local_path", "file_name", "file_size", "mod_time"]

    for dicom_file in file_list:
        file_path_dcm = replace(dicom_file.path, 4, "")
        values.append(
            (
                file_path_dcm,
                dicom_file.name,
                dicom_file.size,
                dicom_file.modificationTime,
            )
        )

    dataframe = spark.createDataFrame(values, columns)
    extract_metadata_udf = udf(extract_metadata, MapType(StringType(), StringType()))
    dicom_dataframe = dataframe.withColumn(
        "metadata_details", extract_metadata_udf(col("local_path"))
    )

    return dicom_dataframe
