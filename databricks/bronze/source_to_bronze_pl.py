# Databricks notebook source
# Install libraries
%pip install pyyaml
%pip install dlt

# COMMAND ----------

# Append the path to the databricks folder to the system path
import sys
sys.path.append('/Workspace/Shared/databricks/')

# COMMAND ----------

# Import libraries
import dlt
import datetime
import pyspark.sql.functions as F
from utilities.op_utilities import get_config, get_schema_from_json

# COMMAND ----------

# Get the pipeline parameters
config_file_path = spark.conf.get("config_file_path")
params = get_config(config_file_path)

# Source to bronze parameters
source_to_bronze_params = params['bronze_ingestion_details']
data_landing_path = source_to_bronze_params["source_file_path"]
inbound_format = source_to_bronze_params["source_file_format"]
# For CSVs
inbound_csv_header = source_to_bronze_params["source_file_first_row_is_header"]
inbound_csv_delimiter = source_to_bronze_params["source_file_delimiter"]
# To do: Decide on schema setting on read
# if source_to_bronze_params["source_file_schema_path"] != "":
#   inbound_csv_schema = get_schema_from_json(source_to_bronze_params["source_file_schema_path"])
# For JSONs
inbound_json_multiline = source_to_bronze_params["source_file_multiline"]
bronze_table = source_to_bronze_params['bronze_table']

# COMMAND ----------

# BRONZE LAYER
# Ingest the raw data to bronze
@dlt.table(name=bronze_table)
def source_to_bronze():
  if inbound_format == 'csv':
    return (
      spark.readStream.format("cloudFiles")
      .option("cloudFiles.format", "csv")
      .option("cloudFiles.inferColumnTypes", True)
      .option("header", inbound_csv_header)
      .option("delimiter", inbound_csv_delimiter)
      .load(data_landing_path)
      .withColumn("bronze_last_modified_dt", F.lit(datetime.datetime.now()))
      )

  elif inbound_format == 'json':
    return (
    spark.readStream.format("cloudFiles")
    .option("cloudFiles.format", "json")
    .option("cloudFiles.inferColumnTypes", True)
    .option("inferTimestamp", True)
    .option("multiLine", inbound_json_multiline)
    .load(data_landing_path)
    .withColumn("bronze_last_modified_dt", F.lit(datetime.datetime.now()))
    )

  elif inbound_format == 'parquet':
    return (
    spark.readStream.format("cloudFiles")
    .option("cloudFiles.format", "parquet")
    .load(data_landing_path)
    .withColumn("bronze_last_modified_dt", F.lit(datetime.datetime.now()))
    )

  elif inbound_format == 'avro':
    return (
    spark.readStream.format("cloudFiles")
    .option("cloudFiles.format", "avro")
    .load(data_landing_path)
    .withColumn("bronze_last_modified_dt", F.lit(datetime.datetime.now()))
    )

  else:
    raise ValueError("Invalid inbound file format specified in the config!")


