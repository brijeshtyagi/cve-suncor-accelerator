# Databricks notebook source
 # Ensure that Great Expectations is installed

 %pip install great-expectations

# COMMAND ----------

# Text parameters:
# Parameter to store the name of schema:
dbutils.widgets.text("schema_name","dbx_accelerator_gold")

# Parameter to store the specific column name & table name:
dbutils.widgets.text("test_id_column_name", "patientID")
dbutils.widgets.text("table_name", "longitudinal_data")


# COMMAND ----------

# Import the spark and great expecation dependencies
import pyspark
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import great_expectations as ge

# Import the ExtendedSparkDFDataset
from great_expectations.dataset import SparkDFDataset
from great_expectations.dataset import MetaSparkDFDataset


# Import the PandaDataset, MetaDataset if needed as below
from great_expectations.dataset import (
    PandasDataset,
    MetaPandasDataset,
)

class RetrievedGoldDataset(SparkDFDataset):
  # """Class that accepts a spark dataset as its incoming dataframe and performs quality 
  #   tests against the finally loaded dataset in the silver layer.   
  # """
  
    _data_asset_type = "GoldDataset"
    tables_details_dict = {}
    patient_columns = []
    table_name = ""

    # create a static method to query through the given table and return the expected dataset.
    @staticmethod
    def returnDatasetForGivenTable(schema_name, table_name):
        return spark.sql(f"SELECT * FROM {schema_name}.{table_name}")
      
# #Verify data quality for expected fields in patients' table
df_dataset = RetrievedGoldDataset.returnDatasetForGivenTable((dbutils.widgets.get("schema_name")), (dbutils.widgets.get("table_name")))
df_gold_table_dataset = SparkDFDataset(df_dataset)                                                             
print(df_gold_table_dataset.expect_column_values_to_not_be_null(dbutils.widgets.get("test_id_column_name")))
