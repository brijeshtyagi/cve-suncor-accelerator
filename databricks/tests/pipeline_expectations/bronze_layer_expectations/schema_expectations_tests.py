# Databricks notebook source
# Ensure that Great Expectations is installed
%pip install great-expectations

# Text parameters:
# Parameter to store the name of schema:
dbutils.widgets.text("schema_name","dbx_accelerator_bronze")

#Dropdown parameters:
# Parameter to store the list of tables in the bronze schema:
bronze_tables_list = ['patients','biosamples', 'encounters', 'labs', 'medications']
dbutils.widgets.dropdown(name='table_name', defaultValue = 'patients', choices=bronze_tables_list)


#-----------------------------------------------------------------------------------------------------------------------------------------------------#
from pyspark.sql import SparkSession
from pyspark.sql.types import TimestampType
from pyspark.sql.types import IntegerType
from pyspark.sql.types import DoubleType
from pyspark.sql.types import StringType
from pyspark.sql.functions import col
from pyspark.sql import SparkSession
import great_expectations as ge
import pandas as pd
import pyspark
import json


# Import the ExtendedSparkDFDataset
from great_expectations.dataset import MetaSparkDFDataset
from great_expectations.dataset import SparkDFDataset

# Import the PandaDataset, MetaDataset if needed as below
from great_expectations.dataset import (
    PandasDataset,
    MetaPandasDataset,
)

class RetrievedBronzeDataset(SparkDFDataset):
  # """Class that accepts a spark dataset as its incoming dataframe and performs schema verification 
  #   tests against the finally loaded dataset in the bronze layer.   
  # """
  
    _data_asset_type = "BronzeDataset"
    given_schema_name = dbutils.widgets.get("schema_name")
    given_table_name = dbutils.widgets.get("table_name")
    # Static dictionary variable to store the values read from the json file
    tables_details_dict = {}
    # Static list variables to store the list of columns available for each table from the json file.
    patients_columns = []
    biosamples_columns = []
    encounters_columns= []
    labs_columns = []
    medications_columns = []

# load data using Python JSON module
with open('../tables.json','r') as f:
    RetrievedBronzeDataset.tables_details_dict = json.loads(f.read())
patients_details = (RetrievedBronzeDataset.tables_details_dict['Tables'][0]['Table1Columns'])
RetrievedBronzeDataset.patients_columns = list(patients_details.keys())
biosamples_details = (RetrievedBronzeDataset.tables_details_dict['Tables'][1]['Table2Columns'])
RetrievedBronzeDataset.biosamples_columns = list(biosamples_details.keys())
encounters_details = (RetrievedBronzeDataset.tables_details_dict['Tables'][2]['Table3Columns'])
#Serialize dict_keys to be used as list
RetrievedBronzeDataset.encounters_columns = list(encounters_details.keys())
lab_details = (RetrievedBronzeDataset.tables_details_dict['Tables'][3]['Table4Columns'])
RetrievedBronzeDataset.labs_columns = lab_details.keys()
medications_details = (RetrievedBronzeDataset.tables_details_dict['Tables'][4]['Table5Columns'])
#Serialize dict_keys to be used as list
RetrievedBronzeDataset.medications_columns = list(medications_details.keys())


#In the below code we are fetching a set of dataset in the tables without any filters. 
   
# Spark Dataframe that retrieves and stores the data queried from given schema and table
df_dataset = (
    spark
        .sql(f"SELECT * FROM {RetrievedBronzeDataset.given_schema_name}.{RetrievedBronzeDataset.given_table_name}")
  )

df_bronze_table_dataset = SparkDFDataset(df_dataset)

# # Verify that expected columns are present in the table
tbName = RetrievedBronzeDataset.given_table_name 

match tbName:
    case "patients":
        for column in RetrievedBronzeDataset.patients_columns:
            print (column)
            print(df_bronze_table_dataset.expect_column_to_exist(column=column))
        
    case "biosamples":
        print(df_bronze_table_dataset.expect_table_columns_to_match_ordered_list(column_list=RetrievedBronzeDataset.biosamples_columns))
        types = (RetrievedBronzeDataset.tables_details_dict['Tables'][1]['Table2Columns'])
        print (types)
        for column, type_ in types.items():
            print(df_bronze_table_dataset.expect_column_values_to_be_of_type(column=column, type_=type_))


    case "encounters":
        print(df_bronze_table_dataset.expect_table_columns_to_match_set(column_set=RetrievedBronzeDataset.encounters_columns))
    
    case "labs":
        for column in RetrievedBronzeDataset.labs_columns:
            print(df_bronze_table_dataset.expect_column_to_exist(column=column))


    case "medications":
        print(df_bronze_table_dataset.expect_table_columns_to_match_ordered_list(column_list=RetrievedBronzeDataset.medications_columns))

# Remove the widgets after the notebook runs successfully
dbutils.widgets.removeAll()
