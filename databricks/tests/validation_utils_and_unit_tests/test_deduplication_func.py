# Databricks notebook source
# MAGIC %pip install pyyaml

# COMMAND ----------

from utilities.op_utilities import deduplicate_data
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType
import pyspark.sql.functions as F
import unittest
import time
import datetime

# COMMAND ----------

class deduplicate_dataTest(unittest.TestCase):
    """test class for the deduplicate_data function"""

    def test_deduplicate_data_func1(self):

        # Define the schema
        schema = StructType(
            [
                StructField("patient_id", IntegerType(), nullable=False),
                StructField("name", StringType(), nullable=False),
            ]
        )

        # Create a list of new records with current timestamps
        data = [(1, "John"), (2, "Jane"), (3, "Alice")]
        self.source_df = spark.createDataFrame(data, schema)
        self.source_df = self.source_df.withColumn(
            "timestamp_column", F.lit(datetime.datetime.now())
        )

        # Update to the existing records with new timestamps
        data = [(2, "Jane"), (3, "Emma")]
        self.new_records_df = spark.createDataFrame(data, schema)
        self.new_records_df = self.new_records_df.withColumn(
            "timestamp_column", F.lit(datetime.datetime.now())
        )

        # DataFrame with updated records and source_df
        self.appended_df = self.source_df.union(self.new_records_df)

        # Function call to return valid records/lastest records
        deduplicate_df = deduplicate_data(
            self.appended_df, ["patient_id"], "timestamp_column", False
        )
        deduplicate_df = deduplicate_df.select("patient_id", "name").orderBy(
            "patient_id"
        )
        expected_data = [(1, "John"), (2, "Jane"), (3, "Emma")]

        expected_df = spark.createDataFrame(expected_data, schema)
        self.assertEqual(deduplicate_df.collect(), expected_df.collect())

        # Function call to return invalid records
        deduplicate_df = deduplicate_data(
            self.appended_df, ["patient_id"], "timestamp_column", True
        )
        deduplicate_df = deduplicate_df.select("patient_id", "name").orderBy(
            "patient_id"
        )

        expected_data = [(2, "Jane"), (3, "Alice")]

        expected_df = spark.createDataFrame(expected_data, schema)
        self.assertEqual(deduplicate_df.collect(), expected_df.collect())


if __name__ == "__main__":
    suite = unittest.TestLoader().loadTestsFromTestCase(deduplicate_dataTest)
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    # Print test results
    if result.wasSuccessful():
        print("All tests passed!")
    else:
        print("Test failures occurred!")
