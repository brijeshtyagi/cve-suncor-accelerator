# Databricks notebook source
# MAGIC %pip install pyyaml

# COMMAND ----------

from utilities.op_utilities import merge_data
from pyspark.sql.functions import expr, col, when
from pyspark.sql import Row
import unittest
import pyspark.sql.functions as F
import datetime

# COMMAND ----------

class MergeDataTest(unittest.TestCase):
    """test class for the Upsert function"""

    def setUp(self):

        self.db = "test_database"
        spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.db}")

        self.target_table = "target_delta_table"

        # Specify the path for the target Delta table
        self.target_table_path = f"{self.db}.{self.target_table}"

        # Create a list of new records
        data = [(1, "John", "New York"), (2, "Jane", "London"), (3, "Alice", "Boston")]

        # Create a DataFrame with the provided data
        columns = ["patient_id", "name", "city"]
        self.source_df = spark.createDataFrame(data=data, schema=columns)

    def tearDown(self):
        # Set the current database
        spark.catalog.setCurrentDatabase(self.db)

        # Delete the tables within the database
        tables = spark.catalog.listTables()
        for tb in tables:
            spark.sql(f"DROP TABLE IF EXISTS {self.db}.{tb.name}")

        # Drop the database
        spark.sql(f"DROP DATABASE IF EXISTS {self.db}")

    def test_mergedata_func_ScdType1_update_certain_columns(self):

        #  Initial load to the target table
        merge_data(
            self.source_df,
            self.target_table_path,
            "upsert",
            "scd1",
            merge_columns=["patient_id"],
            update_columns=["patient_id", "name"]
        )

        self.result_df = spark.table(self.target_table_path)
        self.result_df = self.result_df.select("patient_id", "name", "city").orderBy(
            "patient_id"
        )
        self.expected_df = spark.createDataFrame(
            [(1, "John", "New York"), (2, "Jane", "London"), (3, "Alice", "Boston")],
            ["patient_id", "name", "city"],
        )
        self.assertEqual(self.result_df.collect(), self.expected_df.collect())

        # Update the "name" and "city" columns based on conditions
        self.updated_df = self.source_df.withColumn(
            "name",
            when((col("patient_id") == 1) & (col("name") == "John"), "Jonathan")
            .when(col("patient_id") == 2, "Janet")
            .otherwise(col("name")),
        ).withColumn(
            "city", when(col("patient_id") == 3, "Berlin").otherwise(col("city"))
        )

        # Function call to update specific columns (patient_id and name)
        merge_data(
            self.updated_df,
            self.target_table_path,
            "upsert",
            "scd1",
            merge_columns=["patient_id"],
            update_columns=["patient_id", "name"]
        )

        self.result_df = spark.table(self.target_table_path)
        self.result_df = self.result_df.select("patient_id", "name", "city").orderBy(
            "patient_id"
        )
        self.expected_df = spark.createDataFrame(
            [
                (1, "Jonathan", "New York"),
                (2, "Janet", "London"),
                (3, "Alice", "Boston"),
            ],
            ["patient_id", "name", "city"],
        )
        self.assertEqual(self.result_df.collect(), self.expected_df.collect())

    def test_mergedata_func_ScdType1_update_all_columns(self):

        # Initial load to the target table
        merge_data(
            self.source_df,
            self.target_table_path,
            "upsert",
            "scd1",
            merge_columns=["patient_id"],
            update_columns=[]
        )

        self.result_df = spark.table(self.target_table_path)
        self.result_df = self.result_df.select("patient_id", "name", "city").orderBy(
            "patient_id"
        )
        self.expected_df = spark.createDataFrame(
            [(1, "John", "New York"), (2, "Jane", "London"), (3, "Alice", "Boston")],
            ["patient_id", "name", "city"],
        )
        self.assertEqual(self.result_df.collect(), self.expected_df.collect())

        # Update the "name" and "city" columns based on conditions
        self.updated_df = self.source_df.withColumn(
            "name",
            when((col("patient_id") == 1) & (col("name") == "John"), "Jonathan")
            .when(col("patient_id") == 2, "Janet")
            .otherwise(col("name")),
        ).withColumn(
            "city", when(col("patient_id") == 3, "Berlin").otherwise(col("city"))
        )

        # Passing an empty list to update all column values
        merge_data(
            self.updated_df,
            self.target_table_path,
            "upsert",
            "scd1",
            merge_columns=["patient_id"],
            update_columns=[]
        )

        self.result_df = spark.table(self.target_table_path)
        self.result_df = self.result_df.select("patient_id", "name", "city").orderBy(
            "patient_id"
        )
        self.expected_df = spark.createDataFrame(
            [
                (1, "Jonathan", "New York"),
                (2, "Janet", "London"),
                (3, "Alice", "Berlin"),
            ],
            ["patient_id", "name", "city"],
        )
        self.assertEqual(self.result_df.collect(), self.expected_df.collect())

    def test_merge_data_func_append(self):

        # Initial load to the target table
        merge_data(self.source_df, self.target_table_path, "append")

        self.result_df = spark.table(self.target_table_path)
        self.result_df = self.result_df.select("patient_id", "name", "city").orderBy(
            "patient_id"
        )
        self.expected_df = spark.createDataFrame(
            [(1, "John", "New York"), (2, "Jane", "London"), (3, "Alice", "Boston")],
            ["patient_id", "name", "city"],
        )
        self.assertEqual(self.result_df.collect(), self.expected_df.collect())

        # Update the "name" and "city" columns based on conditions
        self.updated_df = self.source_df.withColumn(
            "name",
            when((col("patient_id") == 1) & (col("name") == "John"), "Jonathan")
            .when(col("patient_id") == 2, "Janet")
            .otherwise(col("name")),
        ).withColumn(
            "city", when(col("patient_id") == 3, "Berlin").otherwise(col("city"))
        )

        merge_data(self.updated_df, self.target_table_path, "append")

        self.result_df = spark.table(self.target_table_path)
        self.result_df = self.result_df.select("patient_id", "name", "city").orderBy(
            "patient_id", "name", "city"
        )
        self.expected_df = spark.createDataFrame(
            [
                (1, "John", "New York"),
                (1, "Jonathan", "New York"),
                (2, "Jane", "London"),
                (2, "Janet", "London"),
                (3, "Alice", "Berlin"),
                (3, "Alice", "Boston"),
            ],
            ["patient_id", "name", "city"],
        )
        self.assertEqual(self.result_df.collect(), self.expected_df.collect())

    def test_mergedata_func_ScdType2(self):
        # Assign the current datetime value
        dt_value = F.lit(datetime.datetime.now())
        #  Initial load to the target table using scdtype2
        merge_data(
            self.source_df, self.target_table_path, "upsert", "scd2", dt_value, ["patient_id"]
        )

        # Load the data from the target table into a DataFrame
        self.result_df = spark.table(self.target_table_path)
        print("Scd Type 2 - First Run")
        display(self.result_df)
        self.result_df = self.result_df.select("patient_id", "name", "city").orderBy(
            "patient_id", "name", "city"
        )
        self.expected_df = spark.createDataFrame(
            [(1, "John", "New York"), (2, "Jane", "London"), (3, "Alice", "Boston")],
            ["patient_id", "name", "city"],
        )
        self.assertEqual(self.result_df.collect(), self.expected_df.collect())

        # Update the "name" and "city" columns based on conditions
        self.updated_df = self.source_df.withColumn(
            "name",
            when((col("patient_id") == 1) & (col("name") == "John"), "Jonathan")
            .when(col("patient_id") == 2, "Janet")
            .otherwise(col("name")),
        ).withColumn(
            "city",
            when(col("patient_id") == 3, "Berlin")
            .when(col("patient_id") == 2, "Chicago")
            .otherwise(col("city")),
        )

        # Create a DataFrame from the new record
        new_record = Row(id=4, name="James", city="Boston")
        new_df = spark.createDataFrame([new_record], ["patient_id", "name", "city"])

        # Add the new record to the existing DataFrame
        self.combined_df = self.updated_df.union(new_df)
        # Assign the current datetime value
        dt_value = F.lit(datetime.datetime.now())
        # Function call to perform scdtype2 with updated data
        merge_data(
            self.combined_df, self.target_table_path, "upsert", "scd2", dt_value, ["patient_id"]
        )

        # Load the data from the target table into a DataFrame
        self.result_df = spark.table(self.target_table_path)
        print("Scd Type 2 - Second Run")
        display(self.result_df)
        self.result_df = self.result_df.select("patient_id", "name", "city").orderBy(
            "patient_id", "name", "city"
        )
        self.expected_df = spark.createDataFrame(
            [
                (1, "John", "New York"),
                (1, "Jonathan", "New York"),
                (2, "Jane", "London"),
                (2, "Janet", "Chicago"),
                (3, "Alice", "Berlin"),
                (3, "Alice", "Boston"),
                (4, "James", "Boston"),
            ],
            ["patient_id", "name", "city"],
        )

        # Assert that the result DataFrame matches the expected DataFrame
        self.assertEqual(self.result_df.collect(), self.expected_df.collect())

        # Update the "name" and "city" columns based on conditions, one record to oberve
        self.updated_df = self.source_df.withColumn(
            "name",
            when((col("patient_id") == 1), "Peter")
            .when((col("patient_id") == 2), "Janet")
            .otherwise(col("name")),
        ).withColumn(
            "city",
            when(col("patient_id") == 3, "Iowa")
            .when(col("patient_id") == 2, "Chicago")
            .otherwise(col("city")),
        )

        # Create a DataFrame from the new record
        new_record = Row(id=5, name="Rob", city="Dallas")
        new_df = spark.createDataFrame([new_record], ["patient_id", "name", "city"])

        # Add the new record to the existing DataFrame
        self.combined_df = self.updated_df.union(new_df)
        # Assign the current datetime value
        dt_value = F.lit(datetime.datetime.now())
        # Function call to perform scdtype2 with updated data
        merge_data(
            self.combined_df, self.target_table_path, "upsert", "scd2", dt_value, ["patient_id"]
        )

        # Load the data from the target table into a DataFrame
        self.result_df = spark.table(self.target_table_path)
        print("Scd Type 2 - Third Run")
        display(self.result_df)
        self.result_df = self.result_df.select("patient_id", "name", "city").orderBy(
            "patient_id", "name", "city"
        )
        self.expected_df = spark.createDataFrame(
            [
                (1, "John", "New York"),
                (1, "Jonathan", "New York"),
                (1, "Peter", "New York"),
                (2, "Jane", "London"),
                (2, "Janet", "Chicago"),
                (3, "Alice", "Berlin"),
                (3, "Alice", "Boston"),
                (3, "Alice", "Iowa"),
                (4, "James", "Boston"),
                (5, "Rob", "Dallas"),
            ],
            ["patient_id", "name", "city"],
        )

        # Assert that the result DataFrame matches the expected DataFrame
        self.assertEqual(self.result_df.collect(), self.expected_df.collect())


if __name__ == "__main__":
    suite = unittest.TestLoader().loadTestsFromTestCase(MergeDataTest)
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)

    # Print test results
    if result.wasSuccessful():
        print("All tests passed!")
    else:
        print("Test failures occurred!")
